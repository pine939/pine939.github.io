---
title: "Google Machine Learning BootCamp (3)"
excerpt: "[Course 1] Neural Networks and Deep Learning - Shallow Neural Networks"

categories:
  - 'ml'
tags:
  - 'data_science'
  - 'ml'

toc: true
toc_sticky: true
sidebar:
  nav: docs
date: 2021-08-18
---

## Shallow Neural Networks

* 정리는 필요하지만 일단 생각나는 것 적기
* 신경망 구조 (3 layers)
* 신경망 계산하기
* Activation Function
* 기울기
* 가중치 벡터 초기화

### Quiz

* denote
  * a[2] : activation vector of the 2nd layer.
  * a[1](10) : activation vector of the 2nd layer for 10th training example.
  * a4[2] : activation output by the 4th neuron(node) of the 2nd layer.
* activation function 
  * tanh은 -1에서 1 사이의 output을 가진다. 이것은 데이터를 중앙에 배치하여 학습을 단순화한다.
  * sigmoid은 0에서 1 사이의 output을 가지기 때문에 binary classification에 적합한 activation function이다.
* forward propogation for layer
  * input layer -> hidden layer -> output layer
* axis
  * axis == 0 : x축 (row)
  * axis == 1 : y축 (column)
  * axis == 2 : z축 (depth)
* initialize zero vs random
  * In NN, if you initialize the w, b to be zero, the NN doesn't work well.
  * 첫 번 째 hidden layer의 노드(뉴런)가 모두 동일한 계산 결과를 가지므로, 이후 계산들은 모두 의미가 없어진다.
* dims
  * w[1] : (n[1], n[0])
  * b[1] : (n[1], 1)
  * z[1] : (n[1], 1)
    * z[1] = w[1]x + b[1]
  * w[2] : (n[2], n[1])
  * b[2] : (n[2], 1)
  * z[2] : (n[2], 1)
    * z[2] = w[2]a[1] + b[2]

### Ian Goodfellow Interview
  * 좋은 코드를 작성하여 github에 공유해 보자.
  * 기계 학습 보안 문제를 생각해 보자.